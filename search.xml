<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200525_Image-to-image%20translation%20for%20cross-domain%20disentanglement%2F</url>
    <content type="text"><![CDATA[Image-to-image translation for cross-domain disentanglementposted on: NIPS2018 In this paper, they combine image translation and domain disentanglement and propose the concept of cross domain disentanglement. Similarly, they separate the latent representation into shared and exclusive parts. The shared part contains information for both domains and the exclusive part contains only factors of variation particular to each domain. Their network contains image translation modules and cross-domain auto-encoders. Image translation modules follow an encoder-decoder architecture. l Given input image, the encoder output the latent representation, which is further separated into shared S and exclusive parts E. To guarantee the correct disentanglement, they apply two ways. 1) Based on the intuition that reconstructing Y images from Ex is impossible, they introduce a small decoder and apply GRL at the beginning layers. With adversarial learning, it forces Ex to contain exclusive features only. 2) to constrain the shared features contains similar information, they apply L1 loss on these features and add noise to avoid small signals. l During disentangling, as higher resolution feature contains both shared and exclusive features, they reduce the bottleneck by increasing the size of the latent representation when encoding shared part and normally apply fully connected layers for exclusive part. l The decoder takes the shared representation and a random noise which serves as the exclusive part as input. To enforce the exclusive features and noises have similar distribution, they adopt a discriminator to push distribution of Ex to N(0,1). And to avoid the noise being ignored, they propose to reconstruct the latent representation using a L1 loss. Cross-domain auto-encoder takes the exchanged shared part and the exclusive part as input and reconstruct the original image by using a L1 loss. This offers an extra incentive for the encoder to put domain specific properties in exclusive representation. Their experiment is conducted mainly on MNIST variations. 1) Without any labels, their model could generate diverse outputs which belongs to the other domain. 2) Given a reference of the other domain, it could also perform domain-specific translation by exchanging the exclusive parts. 3) By interpolating the exclusive and shared representations, it could generate smoothly transformed images. 4) By applying Euclidean distance between features, it could perform cross domain retrieval both semantically and stylistically. All experiments demonstrate the effectiveness of their cross-domain disentanglement. Pros: Though their model is trained on simple dataset MNIST variations, it could be applied to bidirectional multimodal image translation in more complex datasets. It’s not constrained to cross-domain spatial correspondence like pix2pix and BicycleGAN do. Their disentanglement is general and practical. Cons: Though the application of GRL is novel in domain disentanglement, the results in their ablation study indicates that it’s not as useful as they analyzed.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200522_Shapes%20and%20Context%20In-the-Wild%20Image%20Synthesis%20%26%20Manipulation%2F</url>
    <content type="text"><![CDATA[Shapes and Context: In-the-Wild Image Synthesis &amp; Manipulationposted on: CVPR2019 In image synthesis and image manipulation field, recent works are mainly learning-based parametric methods. In this paper, they propose a data-driven model with no learning for interactively synthesizing in-the-wild images from semantic label input masks. Their model is controllable and interpretable, following stages as: (1) global scene context, filter the list of training examples using labels and pixel overlap of labels; (2) instance shape consistency, search boundaries and extract the shapes with similar context; (3) local part consistency, a more fine-grain constrain when global shape is not able to capture, (4) pixel-level consistency, similar to part consistency, fill the remaining holes after (2) and (3). In their quantitative comparison, they measure image realism by applying FID scores and measure image quality by comparing the segmentation outputs between synthesized image and the original. Compared with pix2pix and pix2pix-HD, their method could generate both high-quality and realistic images. In their qualitative comparison，the user study indicates that their results are more favorable than pix2pix. And it could generate diverse outputs without additional efforts. Pros: Compared to other parametric methods, their work has notable advantages: 1) it is not limited to specific training data dataset and distribution, 2) it performs better with more given data, while parametric methods will perform worse, 3) it could generate arbitrarily high-resolution images, 4) it can generate an exponentially large set of viable synthesized images. 5) it’s highly controllable and interpretable. Cons: The synthesized images has a good structure and semantic consistency, but the appearance of different instances is not consistent, making it visual unpleasant.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200521_GeneGAN%20Learning%20Object%20Transfiguration%20and%20Attribute%20Subspace%20from%20Unpaired%20Data%2F</url>
    <content type="text"><![CDATA[GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Dataposted on: BMVC2017 GeneGAN proposed a deterministic generative model which learns disentangled attribute subspaces from weakly labeled data by adversarial training. Fed with two unpaired sets of images (with and without object), GeneGAN uses an Encoder to encode image into two parts: object attribute subspace and background subspace. The object attribute may be eyeglasses, smile, hairstyle and lighting condition. By swapping the object feature input of Decoder, GeneGAN could generate different styles of the same person, such as a person with smile to without smile. Besides reconstruction loss and normal adversarial loss, they also present nulling loss to disentangle object features from background features and the parallelogram loss to enforce a constraint between the children object and the parent objects in image pixel values. Their experiments are conducted on aligned faces. Pros: Compared with CycleGAN, GeneGAN is simpler with only one generator and one discriminator and gains a good performance on face attribute transfiguration in face images from CelebA and Multi-PIE database. The way to learn from weakly labeled unpaired data is inspiring. Two unpaired sets of images that with and without some object is effectively a 0/1 labeling over all training data. Cons: The constraints they presented hold only approximately and there will be potential leakage of information between the object and feature parts. The object feature is not clearly defined. For eyeglasses, it can be the color, type, size, etc. While for hairstyle, it mainly focuses on the hair directions instead of any color information. Maybe it’s following the previous works and I’m wondering.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200520_DRIT%2B%2B%20Diverse%20Image-to-Image%20Translation%20via%20Disentangled%20Representations%2F</url>
    <content type="text"><![CDATA[DRIT++: Diverse Image-to-Image Translation via Disentangled Representationsposted on: ECCV2018 This paper is somewhat like MUNIT, which treat image translation as a one-to-many multimodal mapping with unpaired data. To generate diverse outputs with unpaired training data, they propose a disentangled representation framework for learning, where the input images are embedded onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. 1) To achieve representation disentanglement, they apply two strategies: weight-sharing and a content discriminator. Weight-sharing, similar to UNIT, shares the weight between the last layer of content encoders and the first layer of generators. To further constrain the same content representations encode the same information for both domains, they propose a content discriminator and content adversarial loss, D_c distinguishes the membership of content features while the content decoders try to fool Dc. 2) To address unpair training data, they propose a cross-cycle consistency, which contains two I2I translation: forward and backward translations. In a word, they exchange the attribute representation twice, trying to reconstruct the original images. There are some other loss functions. 1) self-reconstruction loss, reconstruct the original image by encoding and decoding. 2) domain-adversarial loss, encourages G to generate realistic images in each domain. 3) latent regression loss, inspired by BicycleGAN, enforces the reconstruction on the latent attribute vector. 4) KL loss, aim to align the attribute representation with a prior Gaussian distribution 5) mode seeking regularization, improve the diversity. For metrics, they adopt FID to evaluate the image quality, LPIPS to evaluate diversity and JSD&amp;NDB to measure the similarity between the distributions of real images and generated images. Their model could generalize to multi-domain and high-resolution I2I translations. Pros: Though embedding images to content and attribute space is similar to MUNIT, their content adversarial loss guarantees the content space containing no domain-specific information to utmost degree, which is more reasonable than MUNIT. The cross-cycle consistency loss addresses the absence of paired training data in a cyclic way. Their experiments are comprehensive and convincing. Cons: In their user study, there is no detail information about the number of users or the testing images, which is less convincing. And the image quality is much worse than CycleGAN.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020519_Pluralistic%20Image%20Completion%2F</url>
    <content type="text"><![CDATA[Multimodal Unsupervised Image-to-Image Translationposted on: CVPR2019 In this paper, they try to solve image completion in a pluralistic way. That is, given a masked input, the model could generate multiple and diverse plausible outputs, which is quite different to previous methods that could only generate one output. To have a distribution to sample foreground from, they combined CVAE and instance blind and explained why using them directly is infeasible: CVAE learns low variance prior and instance blind is unstable. Therefore, they propose the network with two parallel training paths: 1) reconstruction path is similar as instance blind, trying to reconstruct the original image and get smooth prior distribution of missing foreground. 2) generative path predicts the latent prior distribution for the missing regions conditioned on the visible pixels. During testing, only generative model would be used to infer outputs. The network is based on LS-GAN. For the loss function, they used distribution regularization (KL divergence), appearance matching loss (for rec path, it’s used on the whole image, while for gen path, it’s used on missing foreground only), and adversarial loss. Note that they also present short+long term attention layer, a combination of self-attention layer and contextual flow. Short term attention is placed within decoder to harness distant spatial context. Long term attention is placed between encoder and decoder to capture the feature-feature context. Pros: The mathematical derivation and explanation is clear and full (though I don’t understand some of them) From one-to-one to multi-model/one-to-many is the trend in image-to-image translation and image inpaiting. The short+long attention layer is a good way to attend to the finer-grained features in the encoder or the more semantically generative features in the decoder.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020515_Deep%20Exemplar-based%20Video%20Colorization%2F</url>
    <content type="text"><![CDATA[Multimodal Unsupervised Image-to-Image Translationposted on: CVPR2019 This work is the first exemplar-based video colorization algorithm which is like ‘Deep Exemplar-based Colorization’ except that it is extended to video. It adopts a recurrent structure and also contains two major sub-nets: correspondence subnet and colorization subnet. Compared to image colorization, they must consider temporal consistency besides the color and semantic correspondence. And that’s why they adopt a recurrent structure and take the result of previous frame as input. The loss function is also similar. Besides the perceptual loss and smoothness loss used in [1], they also introduced contextual loss and used adversarial loss and the temporal consistency loss. The contextual loss measures the local feature similarity between the output frame and the reference in a forward matching way. In addition, to degenerate to the common case that the reference comes from the video frames, they add L1 loss to make the output close to the ground truth. They compare to image colorization, auto video colorization and color propagation methods for quantitatively comparison. And they conducted user study for qualitatively comparison.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020514_Multimodal%20Unsupervised%20Image-to-Image%20Translation%2F</url>
    <content type="text"><![CDATA[Multimodal Unsupervised Image-to-Image Translationposted on: ECCV2018 Image-to-image translation is simplified to the problem as a deterministic one-to-one mapping, which makes it difficult to generate diverse outputs from a given source domain image. To address this problem, they extend their previous work UNIT(one-to-one mapping) to multi-model task by combining to BicycleGAN. The image representation is decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To perform translation, they recombine its content code with a random style code sampled from the style space of the target domain. Based on the content code and style code, they propose bidirectional reconstruction loss, including image reconstruction (after encoding to two codes, decodes the original image) and latent reconstruction (same content code as input and same style code as the style). Besides user study, they also use metrics LPIPS and CIS (a modified version of IS). Pros: From 1-to-1 to 1-to-many, the image-to-image translation task is more clear and reasonable. Their assumption of style code and content code is a proper abstract of image-to-image. It is using such an assumption to solve a more challenging problem. Cons: In another paper, they mention that the style code lack many details and is not such beneficial to image-to-image translation?]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020513_Example-Guided%20Style-Consistent%20Image%20Synthesis%20from%20Semantic%20Labeling%2F</url>
    <content type="text"><![CDATA[Example-Guided Style-Consistent Image Synthesis from Semantic Labelingposted on: CVPR2019 In this paper, they present a method for example guided image synthesis with style-consistency from general-form semantic labels. They mainly focus on face, dance and street view image synthesis tasks. Based on pix2pixHD, their network contains 1) a generator, with semantic map x, style example I and its corresponding label F(I) as input and output a synthetic image; 2) a standard discriminator to distinguish real images from fake ones given conditional inputs; and 3) a style consistency discriminator to detect whether the synthetic output and the guidance image I are style-compatible, which operates on image pairs. During network training, they propose to sample style-consistent and style-inconsistent image pairs from video to provide style awareness to the model. They also introduce the style consistency adversarial losses as well as the semantic consistency loss with adaptive weights to produce plausible results. They perform qualitative and quantitative comparison in several applications. Pros: In the past, image-to-image synthesis is difficult to tell whether the network has learned a new data distribution. The style image of this article gives different data distribution types, so this network can output different data distributions. Cons: Though they build their network based on pix2pixHD, they don’t apply that multi-scale architecture and limit the input size to 256*256. The results could be effected significantly by the performance of the state-of-the-art semantic labeling function F (·).]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020510_Toward%20Multimodal%20Image-to-Image%20Translation%2F</url>
    <content type="text"><![CDATA[Toward Multimodal Image-to-Image Translationposted on: NIPS2017 In this paper, they mentioned that in image-to-image translation, a single input may correspond to multiple possible outputs, making it a multi-model problem. So they propose to learn the distribution of possible outputs. They propose a hybrid model BicycleGAN by combining cVAE-GAN and cLR-GAN. cVAE-GAN learns the hidden distribution of image output through VAE, and models the multi-style output distribution. cVAE-GAN starts with ground truth target image B and encodes it into latent space. The generator then attempts to inversely map the input image A along with the sample z to the original image B. cLR-GAN starts with randomly sampled latent code and the condition generator is supposed to produce an output. When the output is used as input to the encoder, the same latent code should be returned to achieve self-consistency. cLR-GAN randomly samples the latent code from a known distribution, uses this code to map A to the output B, and then attempts to reconstruct the latent code from the output. They perform quantitative and qualitative comparison. For quantitative comparison, they measure diversity using average LPIPS distance, and realism using a real vs. fake AMT test. Pros: Their method re-define the image-to-image translation problem in a multi-model way. Combining multiple objectives for encouraging a bijective mapping between the latent and output spaces could address the problem of mode collapse in image generation.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020509_Deep%20Exemplar-based%20Colorization%2F</url>
    <content type="text"><![CDATA[Deep Exemplar-based Colorizationposted on: TOG2018 In this paper, they proposed the first deep learning approach for exemplar-based local colorization, which could directly select, propagate, and predict colors from an aligned reference for a gray-scale image. Their network contains two dub-nets. 1) Similarity sub-net, a preprocessing step which measures the semantic similarity between the reference to the target. Feeding two luminance channels of target and reference to gray VGG-19, they compute cosine distance between the feature maps and output a similarity map. 2) Colorization sub-net, colorization for similar/dissimilar patch/pixel pairs. Taking gray target, aligned reference with chrominance channels and the similarity map, it predicts the ab channels of target image. It contains two different branches with two different loss functions to predict plausible colorization in conditions with and without reliable reference. Chrominance loss in chrominance branch computes smooth L1 distance at each pixel to selectively propagate the correct reference colors. Perceptual loss in perceptual branch minimize the semantic difference between predicted one and target image when the reference is not reliable. To recommend good reference to the user, they also propose an image retrieve algorithm to find a proper reference for a given target. They apply both global ranking (cosine distance between feature maps from the first fully connected layer) and local ranking (cosine distance between feature maps from relu5_2 and correlation coefficient between the illuminance histograms of two local windows) to select proper candidates. Pros: Instead of coloring the image with user strike or by learning from large-scale data, the methods they proposed makes a good balance between controllability from interaction and robustness from learning. For references with proper semantic correspondence, it could propagate correct colors to the target. For those improper reference, they still could generate a plausible result by predict the dominant colors. So it loosen the constrain on references.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020507_Cross-domain%20Correspondence%20Learning%20for%20Exemplar-based%20Image%20Translation%2F</url>
    <content type="text"><![CDATA[Cross-domain Correspondence Learning for Exemplar-based Image Translationposted on: CVPR2020 In this paper, they proposed exemplar-based image translation CoCosNet which learns the dense cross-domain correspondence and outputs images resembling the fine structures of the exemplar at instance level. The cross-domain correspondence and the image translation are jointly learnt with weak supervision because both tasks facilitate each other. Given an exemplar image, they focus on converting a semantic segmentation mask, an edge map, and pose keypoints to a photo-realistic image. CoCosNet has two main sub-network: 1) cross-domain correspondence network, which transforms the inputs from different domains to an intermediate domain. 2) translation network, which progressively synthesizes the output based on the warped exemplar. Take mask to image synthesis as an example. They first align the input semantic image and the input reference style image(exemplar) through the encoder, and use the feature to calculate the similarity between each pixel of the two. After obtain the warped exemplar image according to the similarity, it uses the methods of positional normalization and spatially-variant denormalizaiton (similar to AdaIN) to inject the style into the image during the process of generating the final image from fixed noise z. They apply domain alignment loss and correspondence regularization to guarantee the inputs are aligned to the same domain and the network could learn meaningful correspondence. They also use 1) perceptual loss to minimize the semantic discrepancy, 2) context loss to maintain the same style information (color or texture), 3) feature matching loss to penalize the difference between the translation output and the ground truth for pseudo exemplar pairs, 4) adversarial loss to discriminate the translation output with the real sample of exemplar domain. In their experiment, they select three datasets, ADE20K corresponds to mask-to-image subtask, CelebA-HQ corresponds to edge-to-image subtask and Deepfashion corresponds to keypoints-to-image subtask. They conduct quantitative and qualitative comparisons. For quantitative comparison, they compare in three aspects. 1) image quality: they use two metrics to measure, FID for measuring the distribution distance and SWD for measuring the statistical distance of low-level patch distribution. 2) semantic consistency: they use relu3_2, relu4_2 and relu5_2 of VGG19 to measure this. 3) color and texture distance: they use relu1_2 and relu2_2 to measure this between the semantically corresponding patches. They also present two interesting applications of their work. One is image editing. By manipulating the segmentation layout, it’s feasible to output a translation with the same content manipulation. The other is the makeup transfer. Pros: It’s a general framework for exemplar-based image translation. The development of image translation follows a clear vein, from paired supervision to unsupervised translation, and to multi-model translation. Since then, image translation has been further expanded toward higher resolution, higher quality, video, small sample adaptation, etc. But there are two main problems. 1) The style of the generated image is unpredictable, and the user cannot specify the style of the specific instance; 2) the outputs of existing methods often have obvious artifacts. And their method effectively solves these two problems. They module the input image and the exemplar image as in two distinct domains. And translating images from distinct domains is a general expression, which could generalize to many different kinds of inputs. Their quantitative experiments are comprehensive and the three aspects they consider are very related to their task. Cons: As the translation is based on the warped exemplar, it’s essential that their exemplar has the same semantic labels as the mask for mask-to-image task, which limits the applications. In their ablation study, feature loss shows little improvements. And it seems that L_feat is only applied on pseudo exemplar pairs. So I wonder if it’s necessary to include this loss.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020503_Where%20and%20Who%20Automatic%20Semantic-Aware%20Person%20Composition%2F</url>
    <content type="text"><![CDATA[Where and Who? Automatic Semantic-Aware Person Compositionposted on: WACV2018 In this paper, they proposed an image composition method which focuses on person instance foreground and starts from selecting proper location and size, selecting proper foreground to final compositing. Their method contains three components. 1) using CNN to predict the bounding box, indicating the location and size of the potential person foreground. 2) person segment retrieval, which aims to find a specific segment that semantically matches the local and global context of the background. 3) leveraging alpha matting to make the foreground compatible with the background. In the first step, person instances on COCO images are removed and inpainted with the background. Then after using Faster RCNN object detector to obtain object detection, the layout image and inpained image are fed to predict the normalized coordinates (x_stand, y_stand, w, h) of the bounding box. In the second step, they build a candidate pool using person instances from COCO, filter out those highly occluded ones and manually segment again. To select proper candidates, they compute cosine distance between the global and local feature representations of target segment and candidates. In the last step, the proper candidate is resized according to given bounding box. The alpha matting method is also applied to smooth the transitions. To evaluate the box prediction, they measure the histogram correlation between the predicted and target histogram, including location histogram and size histogram. They conduct ablation studies on their special cases quantitatively and qualitatively. Pros: Most image composition and image harmonization methods focus on appearance consistency of a user selected foreground and a background image. In this work, they focus mainly on predicting candidate person locations and retrieve person instance from candidate pools. The way that uses cosine distance between the global and local feature representations of target segment and candidates to select foreground is similar to our method. It’s a possible solution to few shot problem in other image editing tasks. Cons: It pays little attention to color and illumination consistency. Though the foreground person instance is in a proper location, the composites suffer from lighting inconsistency problems. In the first phase, the results rely on object detector and the following location/size predictor. And the performance of object detector influences the final results significantly.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020502_Toward%20Realistic%20Image%20Compositing%20with%20Adversarial%20Learning%2F</url>
    <content type="text"><![CDATA[Toward Realistic Image Compositing with Adversarial Learningposted on: CVPR2019 Generating a realistic composite image requires both the adjustment of color and illumination and the geometrical transformations. In this paper, based on this observation, they proposed an image composition network consisting of four parts. The transformation network and refinement network act together as the generative compositing model. In this part, they apply spatial transformer network to predict the geometric correction function and linear brightness and contrast model to predict color correction parameters. The refinement network with encoder-decoder architecture is used to deal with boundary artifacts. Adversarial loss is applied to classify real images and generated composite images. They also add an additional segmentation network to predict the foreground mask. To avoid model from removing the foreground during geometric transformation, they propose a geometric loss to penalize large transformations and too small foreground masks. They conducted experiments on synthesized 3-D images and COCO images. For synthesized 3-D images, they generate foreground, background and ground-truth composite images. For COCO, they apply a similar procedure to DIH, processing objects on real images to generate training images. The main difference is that, besides color distortion, they also use another auxiliary mask to simulate the boundary mismatch. They use only user studies to compare between baselines. At the end, they also present to use image manipulation detection model RGB-N to detect different methods, demonstrating the realism of their generated composites. Pros: It a comprehensive method in image compositing field. Geometrical and color consistent adjustment is vital for realistic composites, which matches our intuition. The experiment about image manipulation detection model is also a good way to compare different baselines in image harmonization. Cons: The purpose and effectiveness of additional segmentation network is not so clear. And the effectiveness of refinement network is not presented. Though their model is constructed with many reasonable considerations, their experiments are too simple to test the overall effects. They use only user studies to compare between baselines.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_2020501_Deep%20Painterly%20Harmonization%2F</url>
    <content type="text"><![CDATA[Deep Painterly Harmonizationposted on: Computer Graphics Forum 2018 For local painting transfer, either photographic compositing or global painting transfer performs poorly. In this paper, they proposed a two-pass algorithm to transfer the style of a painting to the photo object pasted on it. The first pass aims for coarse harmonization by first performing a rough match of the color and texture properties. They designed a robust algorithm to deal with different painting styles. The second pass starts from the intermediate result in the first pass and focuses on removing spatial outliers to improve visual quality. More specifically, they adopt pretrained VGG to perform the transfers. In the first pass, they treat each layer independently during input-to-style mapping. The coarsely harmonized results are robust because a poor match in a layer can be compensated for by better matches in the other layers. In the second pass, they enforce consistency across layers and in image space during input-to-style mapping and make some constraints to the style loss. They used style and content loss, histogram loss and total variation loss following previous works. Given a foreground mask, the losses are computed and backpropagated only within the mask. As for the level of stylization varies in different styles of painting, the weights between style and histogram weights need to be considered. So they proposed to train a painting detector to predict the weights and assign to different styles of paintings. Based on the output from the second pass, they also performed a two stage post-processing to deal with medium scale and large scale paintings. The first stage, chrominance denoising, suppresses the highest-frequency color artifacts after converting into CIE-lab color space. The second stage, patch synthesis, filters the base layer and averages the filtered base layer and the detail layer, which contains the high-frequency details. Pros: Style transfer are mostly performed on the whole image, whether between photos or paintings. And transferring such global statistics to local area could lead to artifacts due to some irrelevant regions. They design an effective local approach to perform transfer on local region directly. Cons: The two pass and the painting detector seems reasonable. But with the output from the second pass, they perform a two-stage postprocessing again, making it doubtful whether the second pass makes sense and how much the quality relies on the postprocessing.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200430_Deep%20Image%20Blending%2F</url>
    <content type="text"><![CDATA[Deep Image Blendingposted on: arXiv 2020.4 In this paper, they proposed two-stage blending algorithm, which first seamlessly blends the object onto the background, and then further refine the texture and style of the blending region. To apply Poisson blending, they presented a differentiable Poisson gradient loss to gain the equivalent efforts and easily combine with other losses. Their algorithm doesn’t rely on any training data and could generalize to any source and target images, such as real-world images and styled paintings. Experiments shows that their algorithm outperforms baselines including Poisson blending, style transfer and deep image harmonization. Pros: It tries to solve the main problems faced in many blending/harmonization methods. For Poisson blending, the style and texture inside the foreground region is not consistent with the background. While in style transfer and image harmonization, the boundary consistency remains a problem. In this two-stage algorithm, it focuses on each problem in each stage and could generate visual pleasant results. The Poisson gradient loss they present is reasonable variant of Poisson blending, which could be easily applied in deep learning and combined with other losses. They also use the style loss and the content loss to maintain style consistency and maintain more content information. But different from other content similarity loss [1] or style loss, it’s applied on feature maps, making content close to foreground and style close to background. It uses pretrained VGG to extract feature maps used for style and content loss, and apply a Laplacian filter to compute gradient blending loss, so it doesn’t rely on any training data. Cons: The VGG is pretrained on ImageNet, I wonder whether the feature extracted from a styled painting is reasonable. As there is no ground truth, it could only perform user studies. No quantity comparison between baselines. [1] Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F05%2F26%2FDailyReading_20200429_High-Resolution%20Daytime%20Translation%20Without%20Domain%20Labels%2F</url>
    <content type="text"><![CDATA[High-Resolution Daytime Translation Without Domain Labelsposted on: arXiv 2020 In this paper, they proposed an image-to-image translation model that doesn’t rely on domain labels during training and testing. The model can transfer style from specific image and from a style distribution. They conducted quantitative comparison and qualitative comparison, showing that their model has comparable performance to models requiring labels at least at training time. They also proposed a post-processing model to up-sample the result to a higher resolution. They also demonstrated that their model could be generalized to other domains besides landscapes and also generalized to videos. Pros: Convert video translation task to image-to-image translation. Training with still landscape images and could generate video frames. Compare to previous methods that need labels for training or training and testing, domain labels are not essential here, largely facilitating research. They propose a style distribution loss, which can constrain the style space to be representative and with large diversity. The post-processing model is practical for image generation tasks. And the combination of skip connections and adaptive instance normalization seems good. It’s quite a mixture of components of many state-of-the-art methods. Cons: The model is really complicated with 5 parts and 6 kinds of losses. This task is training and testing specifically on landscape images.]]></content>
  </entry>
  <entry>
    <title><![CDATA[nohup]]></title>
    <url>%2F2019%2F04%2F15%2Fnohup%2F</url>
    <content type="text"><![CDATA[nohupWhen the program is deployed, we hope to keep it running in the background. nohup python test.py &amp; It will return a pid number. To look up the output of the process, you can use tail tail -f nohup.out]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>nohup</tag>
        <tag>background</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2FLatex%20with%20Chinese%20Character%2F</url>
    <content type="text"><![CDATA[Latex with Chinese Character1234567\documentclass&#123;article&#125; \usepackage&#123;CJK&#125; \begin&#123;document&#125; \begin&#123;CJK*&#125;&#123;UTF8&#125;&#123;gkai&#125; 测试中文显示 \end&#123;CJK*&#125; \end&#123;document&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[OpenCV contrib]]></title>
    <url>%2F2019%2F04%2F08%2FOpenCV-contrib%2F</url>
    <content type="text"><![CDATA[OpenCV Contrib is an extension module of OpenCV and it’s also a superset of general OpenCV. Uninstall previous OpenCVIf you have previously installed OpenCV, remove it before installation to avoid conflicts. pip uninstall opencv-python Install OpenCV ContribUse pip to install pre-compiled main and contrib modules (check extra modules listing from OpenCV documentation). pip install opencv-contrib-python Import the packageimport cv2]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mount by SSHFS]]></title>
    <url>%2F2019%2F04%2F07%2FMount-by-SSHFS%2F</url>
    <content type="text"><![CDATA[SSHFS (SSH Filesystem) is a filesystem client to mount and interact with files located on a remote server or workstation over a normal ssh connection. It’s safer and more convenient than nfs. And it’s helpful especially when we are facing multiple servers and large dataset. Installsudo apt-get install sshfs Mount remote filesystem12$ mkdir ~/remoteshared$ sshfs &lt;user&gt;@&lt;host&gt;:/remotepath ~/remoteshared Uninstallumount ~/remoteshared Tips read: Connection reset by peer 123xxx@xxx:~$ sudo sshfs yanhd@172.23.65.122:/home/yanhd ./122read: Connection reset by peerxxx@xxx:~$ Use the ssh port -p &lt;portnumber&gt;. Check whether the ssh works. If ssh works, sshfs will work.]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>remote mount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image Harmonization]]></title>
    <url>%2F2019%2F04%2F06%2FImage-Harmonization%2F</url>
    <content type="text"><![CDATA[Traditional Harmonization MethodsHistogram Matching CG2Real: Improving the Realism of Computer Generated Images Using a Large Collection of Photographs use co-segmentation to segment and match regions in a single step .the content of all images is taken into account during co-segmentation and matching regions are automatically produced as a byproduct of [1] [1] Cosegmentation of Image Pairs by Histogram Matching- Incorporating a Global Constraint into MRFs mean-shift framework [2] [2] The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition feature vector: concatenation of the pixel color in L a b* space, the normalized x and y coordinates at p, and a binary indicator vector (i0; . . . ; ik) such that ij is 1 when pixel p is in the jth image and 0 otherwise a combination of joint-bilateral up-sampling and local color transfer related topics alpha matting: Fuse image by combining their absolute pixel values. The color of the image are linearly interpolated using weights specified by the alpha matte. gradient domain compositing image blending Poisson blending image pyramids Laplacian pyramids]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Image Harmonization</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
</search>
